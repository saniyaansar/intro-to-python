{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Tools in Python\n",
    "-----\n",
    "In this tutorial, you will learn about available software packages in Python which are used by data scientists. The ideal persona for this tutorial is a person who has never used Python and is curious to explore whats available. The idea here is to introduce you to available resources in Python rather than teach you details on how to use as a specific package.  For selected packages, a simple example will be provided to demonstrate usage. Otherwise, most packages will be listed for the leaner to explore in their own time. However, we will use some of the packages in later sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Learning Outcomes](#Learning Outcomes)\n",
    "- [General Purpose Data Wrangling](#General Purpose Data Wrangling)\n",
    "    - [Pandas](#Pandas)\n",
    "    - [Numpy](#Numpy)\n",
    "    - [Apache Spark for Large scale data processing](#Apache Spark for Large scale data processing)\n",
    "- [Web Scraping](#Web Scraping)\n",
    "- [Natural Language Processing](#Natural Language Processing)\n",
    "    - [NTLK](#Navigation)\n",
    "- [Geospatial Data](#Geospatial Data)\n",
    "    - [GDAL](#GDAL)\n",
    "    - [Geopandas](#Geopandas)\n",
    "    - [Shapely](#Shapely)\n",
    "    - [Rasterio](#Rasterio)\n",
    "- [Statistical Analysis and Optimisation](#Statistical Analysis and Optimisations)\n",
    "    - [scipy](#scipy)\n",
    "    - [Numpy](#Numpy)\n",
    "    - [statsmodels](#statsmodels)\n",
    "- [Machine Learning](#Machine Learning)\n",
    "    - [sciki-learn](#sciki-learn)\n",
    "    - [Tensorflow](#Tensorflow)\n",
    "    - [Theano](#Theano)\n",
    "- [Data Visualization](#Data Visualization)\n",
    "    - [Matplotlib](#Matplotlib)\n",
    "    - [Seaborn](#SNS)\n",
    "    - [Bokeh](#Bokeh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outomes\n",
    "After going through this notebook, the leaner should:\n",
    "- Be familiar with  Python packages for the following purposes:\n",
    "    - Data wrangling\n",
    "    - Statistical data analysis\n",
    "    - Data visualization\n",
    "    - Natural language processing\n",
    "    - Geospatial data processing and visualization\n",
    "    - Machine Learning\n",
    "- Appreciate the Python package ecoystem for data science\n",
    "- Use import statement to import packages and perfom simple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Purpose Data Wrangling\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this category, we look at Python libraries which can be used to perfom the most common data wrangling tasks such as data ingestion, data cleaning, subsetting data, recoding variables, checking for missing values, imputing missing values, exploring distributions and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Pandas is a library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. Pandas is free software released under the three-clause BSD license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion with Pandas: A Case of CSV file\n",
    "Pandas can work with many data stores and file formats. To take a quick look at what file formats pandas can read, type ```pd.read``` and then hit tab, you should see a list of all the file formats supported by pandas. In this example, we show reading from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas package \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the name of the CSV file\n",
    "data_file = \"/Users/dmatekenya/Google-Drive/gigs/un-siap-2018/ch2-python-intro/data/power-outages.csv\"\n",
    "\n",
    "# Read data into dataframe\n",
    "# dataframe is the pandas object for handling tabular data\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "Once you have read the data, pandas has many functions to allow you explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>psu</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>str_datetime_sent_hr</th>\n",
       "      <th>power_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>69.075835</td>\n",
       "      <td>38.563905</td>\n",
       "      <td>11/22/16 22:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>69.945693</td>\n",
       "      <td>40.432452</td>\n",
       "      <td>11/22/16 22:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140</td>\n",
       "      <td>67.627770</td>\n",
       "      <td>39.499137</td>\n",
       "      <td>11/22/16 22:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>69.756089</td>\n",
       "      <td>38.355409</td>\n",
       "      <td>11/22/16 22:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>70.090710</td>\n",
       "      <td>38.995396</td>\n",
       "      <td>11/22/16 22:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   psu        lon        lat str_datetime_sent_hr  power_state\n",
       "0   35  69.075835  38.563905       11/22/16 22:00            0\n",
       "1  113  69.945693  40.432452       11/22/16 22:00            1\n",
       "2  140  67.627770  39.499137       11/22/16 22:00            0\n",
       "3   97  69.756089  38.355409       11/22/16 22:00            0\n",
       "4   47  70.090710  38.995396       11/22/16 22:00            0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the n-top rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes and other info about the columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics if it makes sense\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps with Pandas\n",
    "Please refer to pandas [documentation](http://pandas.pydata.org/pandas-docs/stable/10min.html) for tutorials on how to perfom various tasks such as indexing rows, subsetting the data, chaning column names and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**Numpy** is one of the underlying libraries which powers many high level packages such as pandas. In most case, you will not need to interact with **Numpy** directly but its an essential package for data manipulation and scientific computing in Python. Its useful for linear algebra, Fourier transform, and random number capabilities because it has advanced array/matrix functionalities. Also, most of the machine learning packages do require input as **Numpy** arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays in Numpy\n",
    "In the code below, we show how **Numpy** arrays work seamlessly with pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# create two arrays\n",
    "x = np.array([i + 100 for i in range(10)])\n",
    "y = np.array([i -10 for i in range(10)])\n",
    "\n",
    "# create a dictionary with key as column name and value as the numpy arrays\n",
    "data = {\"x\": x, \"y\":y}\n",
    "\n",
    "# Use the dictinary to create a pandas dataframe\n",
    "df_from_np = pd.DataFrame(data)\n",
    "\n",
    "# Check out the dataframe\n",
    "df_from_np.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out Numpy Documentation for Additional Details\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "[Numpy documentation](https://docs.scipy.org/doc/numpy/user/quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallell Data Processing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When you have large datasets regular packages such as pandas do not scale very well. For instance, \n",
    "even a 5gb dataset cant be handled gracefully by pandas. When you have these large datasets, \n",
    "you look for packages which can parellilze processing and make things faster. In Python, they \n",
    "are many tools for parallell data processing as follows:\n",
    "- [Apache Spark](https://spark.apache.org): this probably the most popular framework for distributed data processing\n",
    "- [Dask](http://docs.dask.org/en/latest/why.html): This library offers code parallelisation \n",
    "    utilising existing libraries such as pandas\n",
    "- [Python Multiprocessing Library](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process): Python offers a multiprocessing library but it requires people \n",
    "    who are ore experienced with programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Power of Apache Spark\n",
    "In the example below, lets see how fast things can get when we use APache Spark compared to Pandas. Please pay attention to the time taken to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for measuring execution \n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def timefn(fn):\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add @timefn to measure execution time of the function\n",
    "@timefn\n",
    "def load_big_csv_with_apache_spark(big_csv=None):\n",
    "    \"\"\"\n",
    "    A simple function which loads a CSV file using Apache Spark and\n",
    "    then counts how many rows are in the file\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.master(\"local[12]\").appName(\"data_processor\").getOrCreate()\n",
    "    df = spark.read.csv(big_csv)\n",
    "    cnt = df.count()\n",
    "    print('Number of rows in big CSV file: {}'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import APache Spark relevant library\n",
    "# in windows, add findspark\n",
    "# import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# the task is to read in an 8GB CSV file and then count number of rows\n",
    "big_csv_file = os.path.abspath(\"/Users/dmatekenya/Google-Drive/gigs/un-siap-2018/ch3-big-data-processing/data/activity_log_raw.csv\")\n",
    "load_big_csv_with_apache_spark(big_csv=big_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Write a function which reads the same data using pandas package**\n",
    "- Add @timefn at beginning of function to time it\n",
    "- Refer to previous cells to see how to read a CSV with pandas\n",
    "- Use the *shape* function for pandas to get the dimensions of the dataframe\n",
    "- Compare the time taken for pandas to read in the file and count the rows with that of Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We often have to deal with data which is geospatial in nature or has geographic coordinates. Python has a suite of tools for handling spatial data as we will show in the next section.\n",
    "- [GDAL](https://www.gdal.org): this is one of the core libraries for provising geospatial data functionality. Most of the other libraries are based on GDAL.\n",
    "- [Geopandas](http://geopandas.org): A pandas version for geospatial data\n",
    "- [Shapely](https://shapely.readthedocs.io/en/stable/manual.html): yet another spatial data library with alot of functionalities.\n",
    "- [Rasterio](https://rasterio.readthedocs.io/en/latest/quickstart.html): this library fosues on raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Shapefiles with GDAL OGR Library\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "GDAL s a translator library for raster and vector geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. Its also used in oen source GIS software such as QGIS. In Python, you can use the library to read raster and vector data as well as manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "from osgeo import ogr\n",
    "\n",
    "# shapefile path\n",
    "shp_file = os.path.abspath(\"/Users/dmatekenya/Google-Drive/gigs/un-siap-2018/ch2-python-intro/data/Balaka_EA.shp\")\n",
    "\n",
    "# open shapefile\n",
    "file = ogr.Open(shp_file)\n",
    "shape = file.GetLayer(0)\n",
    "\n",
    "#first feature of the shapefile\n",
    "feature = shape.GetFeature(0)\n",
    "\n",
    "# Convert first feature to GeoJSON\n",
    "first = feature.ExportToJson()\n",
    "\n",
    "# print the JSON object\n",
    "print(first) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Spatial Data with Geopandas\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "GeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# read the shapefile which we arleady defined in cells above\n",
    "df = gpd.read_file(shp_file)\n",
    "\n",
    "# diplay the shapefile \n",
    "display(df.plot(figsize=(8, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Geospatial Data Libraries\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "They are other essential packages to handle spatial data in Python including:\n",
    "- Fiona: Geopandas uses this library behind the scenes\n",
    "- Descartes: For spatial data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 4: ** Use GDAL out find number of features in the shapefile we opened above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis and Optimisations\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If you are interested in more sophisticated statistical analysis, Python also offers further tools in addtion to packages like Pandas which also has basic statistical functionalities. Here, we look at the following packages:\n",
    "- [Statsmodels](https://www.statsmodels.org/stable/index.html)\n",
    "- [Scipy](): This is part of the Python scientific computing stack which also includes Numpy\n",
    "- [Numpy](): Arleady mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Estimation with StatsModels\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.Refer to [documentation](https://www.statsmodels.org/stable/index.html) for further details. The example　below has been adopted from StatsModels documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "nsample = 1000\n",
    "x = np.linspace(0, 10, 1000)\n",
    "X = np.column_stack((x, x**2))\n",
    "beta = np.array([1, 0.1, 10])\n",
    "e = np.random.normal(size=nsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python has become the lingua franca for many data science applications including Machine Learning. It therefore comes as no surprise that its one of the leading scripting langueages when it comes to machine learning. The top machine learning packages in Python based on usage include:\n",
    "- scikit-learn: This is a general purpose machine learning library and it can be used for diverse machine tasks unlike other libraries which may be more specialised. \n",
    "- Tensorflow: this library is focused on deep learning and other neural networks approaches\n",
    "- Theono: Also focused on deep learning and neural networks\n",
    "\n",
    "We will spend 2 hours looking at building a simple ML algorithm in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A lot of unstructured data is available on the internet. Once gathered and put into a structured & meaningful format, this data can be used to perform analytics and derive meaningful insights. In this tutorial, I am going to demonstrate how we can collect such data from the internet and process it to get it to be in a structured format. This process is known as Web Scraping. It is also called Screen Scraping, Web Harvesting or Web Data Extraction. In Python, they are many packages for web scraping but the most common ones include:\n",
    "- requests: A library that fetches the content of the URL.\n",
    "- Beatifulsoup: A library that allows you to parse the HTML source code in a beautiful way.\n",
    "- Scrapy: AN out of the box solution for web scraping with other advanced functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sneak Peak at Requests\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use request to get contents od webpage\n",
    "r = requests.get(\"https://www.kdnuggets.com/2018/02/top-news-week-0129-0204.html\")\n",
    "c = r.content\n",
    "\n",
    "# And BS to parse the html\n",
    "soup = BeautifulSoup(c,\"html.parser\")\n",
    "\n",
    "# we print a heading three tag\n",
    "most_pop_last_week = soup.find('h3').text\n",
    "print(most_pop_last_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5: Use requests to download content from your favorite website and print the contents as we did in cell above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Simply and in short, natural language processing (NLP) is about developing applications and services that are able to understand human languages. We are talking here about practical examples of natural language processing (NLP) like speech recognition, speech translation, understanding complete sentences, understanding synonyms of matching words, and writing complete grammatically correct sentences and paragraphs.\n",
    "\n",
    "In Python, there are many libraries for dealing with language and textual data, we preview some of them:\n",
    " - Natural language toolkit (NLTK): A library for general purpose tect procesing \n",
    " - TextBlob\n",
    " - spaCy\n",
    " - Stanford Core NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A taste of NLTK on Twitter Data\n",
    "Lets quickly look at this twitter dataset from [datahack](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/) just to demonstrate how language processing is possible in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we have to import the package\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# When running for the first time, please uncomment line beloe to donwload the extra packages\n",
    "# nltk.download('punkt') \n",
    "\n",
    "\n",
    "# path to data file\n",
    "twitter_data = os.path.abspath(\"/Users/dmatekenya/Google-Drive/gigs/un-siap-2018/ch2-python-intro/data/twitter-dataset-from-datahack.csv\")\n",
    "\n",
    "# Get CSV\n",
    "df_twitter = pd.read_csv(twitter_data)\n",
    "\n",
    "# One preprocessing steps in NLP is tokenizing (converting sentences into separate words)\n",
    "df_twitter['tokens'] = df_twitter.apply(lambda x: nltk.word_tokenize(x['tweet']), axis=1)\n",
    "\n",
    "# Lets check the split tweets\n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Translation with TextBlob\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Translation is one of the common tasks in NLP. Many APIs provide translation, for example, you can use Google API to tranlate texts on demand for free or for a charge depending on the bulk of your translations. However, in Python the NLP libraries also enable you to translate as we will see in exmaple below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "chinese_blob = TextBlob(u\"美丽优于丑陋\")\n",
    "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6: Using TextBlob just like in the example above, translate from English to your language**\n",
    "- Translate *I love you* to your local language or a language you understand other than English\n",
    "- Note that the package uses languaage codes instead full language names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python has many data visualisation tools, here are some of them:\n",
    "- [Matplotlib]() : this is the base plotting engine in Python. Most of the other packages are somehow based on it.\n",
    "- [Seaborn](): A relatively new packahe with more colourful plots\n",
    "- [Bokeh](): this is an interactive visualization library that targets modern web browsers for presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualisations with Bokeh\n",
    "\n",
    "The simple interactive example below has been copied from [Bokeh Documentation] (https://demo.bokehplots.com/apps/moviesfrom) for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "N = 4000\n",
    "x = np.random.random(size=N) * 100\n",
    "y = np.random.random(size=N) * 100\n",
    "radii = np.random.random(size=N) * 1.5\n",
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)\n",
    "]\n",
    "\n",
    "TOOLS=\"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\"\n",
    "\n",
    "p = figure(tools=TOOLS)\n",
    "\n",
    "p.scatter(x, y, radius=radii,\n",
    "          fill_color=colors, fill_alpha=0.6,\n",
    "          line_color=None)\n",
    "\n",
    "output_file(\"color_scatter.html\", title=\"color_scatter.py example\")\n",
    "\n",
    "# please inspect the plot in the browser\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this tutorial, we previewed some of the essential packages in Python for doing data science with the intent to get excited to learn more. In the next tutorials, we will delve more in some selected areas as follows:\n",
    "- Building Machine Learning Models\n",
    "- Big Data Processing with Apache Spark\n",
    "- Data visualization\n",
    "\n",
    "You can always learn more about any of the packages mentioned here by going to the documentation page linked or doing a Google search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "326px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
